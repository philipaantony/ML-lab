" 

 
import numpy as np
arr = np.array([3, 2, 1, 2])
print(""Original array: "", arr)
print(""Append (6,7,8): "", np.append(arr, [6, 7, 8]))
print(""Insert Specific (10,11) at third second position: "", np.insert(arr, 2, [10, 11]))
print(""Delete values (1,3): "", np.delete(arr, [0, 2]))
print(""Unique element: "", np.unique(arr))
print(""Sorted array: "", np.sort(arr))
np.savetxt('arr.txt', arr)
ld = np.loadtxt('arr.txt')
print(""Loaded from arr.txt: "", ld)




import numpy as np
arr1 = np.array([1, 2, 3, 4, 5])
arr2 = np.array([6, 7, 8, 9, 10])
result_add = arr1 + arr2
print(""Sum of array: "", result_add)
result_multiply = arr1 * arr2
print(""Product of array: "", result_multiply)
print(""Mean of array: "", np.mean(result_add))
print(""Max value: "", np.max(result_multiply))




import numpy as np
grades = np.array([85, 90, 78, 92, 88, 76, 95, 89, 84, 91])
print(""Average grade: "", np.mean(grades))
filter_grade = grades[grades > 90]
print(""Number of Students scoring above 90: "", len(filter_grade))
std_grade = np.std(grades)
print(""Standard deviation of grades: "", np.round(std_grade, decimals=2))






import numpy as np
from scipy.linalg import svd
matrix_A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
matrix_B = np.array([[9, 8, 7], [6, 5, 4], [3, 2, 1]])
matrix_sum = matrix_A + matrix_B
print(""Sum of matrices: \n"", matrix_sum)
matrix_product = matrix_A * matrix_B
print(""Product of matrices: \n"", matrix_product)
matrix_dot = np.dot(matrix_A, matrix_B)
print(""Dot Product of matrices: \n"", matrix_dot)
matrix_A_transpose = np.transpose(matrix_A)
print(""Transpose of matrix A: \n"", matrix_A_transpose)
determinant_B = np.linalg.det(matrix_B)
print(""Determinant of matrix B: "", determinant_B)
eigenvalues_A, eigenvectors_A = np.linalg.eig(matrix_A)
print(""Eigenvalues of matrix A: \n"", eigenvalues_A)
print(""Eigenvectors of matrix A: \n"", eigenvectors_A)
U, s, VT = svd(matrix_A)
print(""SVD of Martrix A: "")
print(""U: "", U)
print(""s: "", s)
print(""VT: "", VT)







import pandas as pd
df = pd.read_csv(""sales_data.csv"")
print(df.head())
print(""Number of rows: "", len(df))
print(""Number of columns: "", len(df.columns))
print(""Total Revenue: "", sum(df['Revenue']))





import pandas as pd
df = pd.read_csv(""student_data.csv"")
print(df.head(), ""\n"")
age = df[df['Age'] > 19]
print(""Students who are 20 years old or older: \n"", age)
print(""\nAverage GPA of all students:"",df['GPA'].mean().round(2))
data1 = df.sort_values(by='GPA', ascending =  False)
print(""\nTop 5 students with hightest GPA: \n"", data1.head(5))
data2 = df.groupby('Age')['GPA'].mean().reset_index()
print(""\nAverage GPA by age group: \n"", data2)


























import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
marks = np.array([10,18,34,
                37, 33, 38,
                34, 24, 80,
                45, 49, 27,
                31, 35, 42])
fig, ax = plt.subplots(figsize = (4, 4))
ax.hist(marks, color = ""darkcyan"", ec = ""black"", lw = 1)
plt.title('Histogram: Exam Score')
plt.ylabel('No. of students')
plt.xlabel('Score')
plt.figure(figsize = (4, 4))
sns.boxplot(y = marks, color = 'darkcyan')
plt.title('Quartile Plot: Exam Score')
plt.ylabel('Score')
plt.show()







import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

x_values = [1, 2, 3, 4, 5]
y_values = [10, 15, 13, 18, 20]
data_values = [10, 15, 13, 18, 20]
df = pd.DataFrame({'x': x_values, 'y': y_values, 'value': data_values})
heatmap_data = df.pivot_table(index = 'x', columns = 'y', values = 'value')
fig, ax = plt.subplots(figsize = (10, 6))
sns.heatmap(heatmap_data, annot = True, cmap = 'YlGnBu', cbar = True)
plt.title('Heatmap Example')
plt.show()

Scatter Plot:-

x_values = [1, 2, 3, 4, 5]
data_values = [10, 15, 13, 18, 20]

df = pd.DataFrame({'x': x_values, 'value': data_values})

plt.scatter(df['x'], df['value'], marker = 'o', color = 'blue', label = 'Data Points')
plt.xlabel('No. of students')
plt.ylabel('Values')
plt.title('Scatter Plot based on Heatmap Values')
plt.show()






 
 



Density Chart:-
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

data = np.random.randn(1000)

sns.kdeplot(data, fill=True, color='blue', label='Density Plot')

plt.xlabel('X-Axis Label')
plt.ylabel('Density')
plt.title('Density Plot Example')

plt.show()

Bubble Diagram
import matplotlib.pyplot as plt

x = [1, 2, 3, 4, 5]
y = [10, 15, 13, 18, 20]
sizes = [100, 200, 300, 150, 250]
plt.scatter(x, y, s=sizes, alpha=0.5)

plt.xlabel('X-Axis')
plt.ylabel('Y-Axis')

plt.title('Bubble Chart Example')
plt.show()




from sklearn.neighbors import KNeighborsClassifier 
from sklearn.model_selection import train_test_split 
from sklearn.datasets import load_iris
from sklearn.metrics import accuracy_score 
iris = load_iris()
x = iris.data 
y = iris.target
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42) 
knn = KNeighborsClassifier(n_neighbors = 7)
knn.fit(x_train, y_train) print(knn.predict(x_test)) 
V = knn.predict(x_test) 
result = accuracy_score(y_test, V) print(""Accuracy= "", result)






import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score  
data = pd.read_csv('Salary_Data.csv') 
x = data['YearsExperience'].values.reshape(-1, 1) 
y = data['Salary'].values
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42) 
LR = LinearRegression()
LR.fit(x_train, y_train) 
D = LR.predict(x_test) 
r2 = r2_score(y_test, D) print(""R2 Score: "", r2)
plt.scatter(x_test, y_test, color = 'black', label = 'Data Points') plt.plot(x_test, D, color = 'blue', linewidth = 3, label = 'Regression Line') plt.xlabel = 'YearsExperience'
plt.ylabel = 'Salary' plt.legend() plt.show()


import pandas as pd
from sklearn.datasets import fetch_california_housing from sklearn.metrics import mean_squared_error 
from sklearn.model_selection import train_test_split 
from sklearn.linear_model import LinearRegression california_housing = fetch_california_housing()
df = pd.DataFrame(data = california_housing.data, columns = california_housing.feature_names) df['Target'] = california_housing.target
x = df.drop('Target', axis = 1) 
y = df['Target']
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)
model = LinearRegression()
model.fit(x_train, y_train) 
predictions  =  model.predict(x_test)
mse  =  mean_squared_error(y_test,  predictions) 
print(f ""Mean Squared Error: {mse}"")





from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split 
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, classification_report 
from matplotlib import pyplot as plt
iris = load_iris() 
x = iris.data 
y = iris.target
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state = 42) 
dt = DecisionTreeClassifier(max_depth = 3)
dt.fit(x_train, y_train) print(dt.predict(x_test)) 
D = dt.predict(x_test) 
result = accuracy_score(y_test, D) print(""Accuracy = "", result) 
cr = classification_report(y_test, D) print(""Classification Report: "", cr) plt.figure(figsize = (15, 20))
plot_tree(dt, filled = True, feature_names = iris.feature_names, class_names = iris.target_names) plt.title(""Decision Tree"")
plt.show()










from sklearn.datasets import  load_iris 
from sklearn.cluster import KMeans import matplotlib.pyplot as plt
iris = load_iris() 
x = iris.data
y = iris.target
kmeans = KMeans(n_clusters = 3,  random_state = 42) kmeans.fit(x)
cluster_labels = kmeans.labels_ print(cluster_labels)
centroids = kmeans.cluster_centers_ print(centroids)
plt.scatter(x[:, 0], x[:, 1], c = cluster_labels, cmap = 'viridis', marker = 'o', edgecolors = 'black') plt.scatter(centroids[:, 0], centroids[:, 1], marker = ""*"", s = 200, c = 'red', label = 'Centroids') plt.xlabel(iris.feature_names[0])
plt.ylabel(iris.feature_names[1]) plt.title('KMeans Cluster of Iris Dataset') plt.legend()
plt.show()





import nltk
from nltk import pos_tag
from nltk.tokenize import word_tokenize

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

# Sample sentence
sentence = ""Cat, Dog, House""

# Tokenize the sentence
words = word_tokenize(sentence)

# Perform POS tagging
pos_tags = pos_tag(words)

# Display the result
print(pos_tags)


Output Screenshot

 



from nltk import bigrams,word_tokenize
sentence = ""Hello Have a Good Day""
words = word_tokenize(sentence)
bigrams_list = list(bigrams(words))
print(bigrams_list)














import requests
from bs4 import BeautifulSoup
def simple_web_crawler(url, max_depth = 2):
  visited_urls = set()
 def crawl(url, depth)
 if depth > max_depth or url in visited_urls:
            return
 print(f ""Crawling: {url}"")
  try:
            response = requests.get(url)
            visited_urls.add(url)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                title = soup.title.string.strip() if soup.title else 'No title found'
                print(f""Page Title: {title}"")
                for link in soup.find_all('a', href = True):
                    next_url = link['href']
                    crawl(next_url, depth + 1)
        except Exception as e:
            print(f ""Error crawling {url}: {e}"")
    crawl(url, depth = 1)
if __name__ == ""__main__"":
    start_url = ""https://aesajce.in""
    simple_web_crawler(start_url)




import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.datasets import load_iris
from tensorflow.keras import models, layers
# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target
# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# One-hot encode the target variable
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_test = label_encoder.transform(y_test)
# Create a simple feedforward neural network model
model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(4,)))
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dense(3, activation='softmax'))  # Output layer with 3 classes
# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',  # Use 'categorical_crossentropy' if y is one-hot encoded
              metrics=['accuracy'])
# Train the model
model.fit(X_train, y_train, epochs=50, batch_size=8, validation_split=0.1)
# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f 'Test accuracy: {test_acc}')



"
